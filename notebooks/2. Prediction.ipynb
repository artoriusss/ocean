{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *13. Describe or show how you would create a Machine Learning Model to predict “dwell” times for the region.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crucial aspect here is to prepare the dataset correctly. Our goal is to predict dwell times, so before even trying to do that, we need to obtain this feature (that is, calculate dvelling time for each vessel by `mmsi` identifier). \n",
    "\n",
    "It is also important to select and leave a reasonable list of features for model training. For example, at the previous stages we've learnt that dwelling time can differ depending on navigation description. Lots of features, on the other hand, are irrelevant. Or in particular, irrelant in our context: our data represents a shipments from a single port, so it does not make a lot of sense to choose location parameters (lat, lon) for training because they are nearly identical and do not carry any valuable meaning. The same applies to the port name and so forth. \n",
    "\n",
    "Let's now get our hands dirty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/parquet')\n",
    "\n",
    "nav_details = pd.DataFrame(df['navigation'].tolist())\n",
    "vessel_details = pd.DataFrame(df['vesselDetails'].tolist())\n",
    "posirion = pd.DataFrame(df['position'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop irrelevant features (or those packed in JSON structure some of which will return in a different format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['epochMillis', 'mmsi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['epochMillis'], unit='ms')\n",
    "df['navCode'] = nav_details['navCode']\n",
    "\n",
    "df = df.sort_values(by=['mmsi', 'timestamp'])\n",
    "df['lead_time'] = df.groupby('navCode')['timestamp'].diff().shift(-1).dt.total_seconds() / 60 # minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect is dealing with outliers. Our dataset is big enough so that we might (for a starting experimental point) simply try to remove extreme outliers (those that are beyond below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444342, 5)\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['lead_time'].quantile(0.25)\n",
    "Q3 = df['lead_time'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['lead_time'] < (Q1 - 1.5 * IQR)) | (df['lead_time'] > (Q3 + 1.5 * IQR))]\n",
    "print(outliers.shape)\n",
    "\n",
    "df = df[~((df['lead_time'] < (Q1 - 1.5 * IQR)) | (df['lead_time'] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed 444342 outliars, which may seem a lot at first glance, taking into account the size of the whole dataset, this is only about 13%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['typeName'] = vessel_details['typeName']\n",
    "df['navDesc'] = nav_details['navDesc']\n",
    "df['courseOverGround'] = nav_details['courseOverGround']\n",
    "df['speedOverGround'] = nav_details['speedOverGround']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['lead_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four features that I decided to select are navigation description, course over the ground, speed over the ground, and vessel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['navDesc', 'courseOverGround', 'speedOverGround', 'typeName']]\n",
    "y = df['lead_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Categorial Variables (One-Hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['navDesc', 'typeName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "def evaluate(y_pred, y_test):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'R-squared (R²): {r2}')\n",
    "    print(f'Median Absolute Error: {medae}')\n",
    "\n",
    "    return mse, mae, medae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 22.933907487749178\n",
      "Mean Absolute Error (MAE): 3.3828172567205357\n",
      "R-squared (R²): 0.04388435618894415\n",
      "Median Absolute Error: 1.6950060070939053\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the metrics, we can see that our model on average is wrong by about 3 minutes in dvell time. $R^2$ is quite low indicating that the model explains only a small portion of the variance in the target variable. Finally, Median Absolute Error being lower than MAE indicates that most errors are relatively small, but there are some larger errors that affect the MAE (there are still outliers that affect the performance). Summarising, this model has the potential to be improved by more extended feature engineering and dealing with outliers. It wouldn't hurt to also experiment with different and more complex models (such as Random Forests or XGBoost)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
